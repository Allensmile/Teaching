{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " # Introduction to Bayesian Optimization\n",
    "\n",
    "*February 2017*\n",
    "\n",
    "**Michiel Stock**\n",
    "\n",
    "michielfmstock@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sympy as sp\n",
    "sp.init_printing()\n",
    "from numpy import sin, exp, cos\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.stats import norm\n",
    "from scipy.integrate import odeint\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Goal**:\n",
    "\n",
    "Solve \n",
    "\n",
    "$$\n",
    "\\mathbf{x}^* = \\text{arg min}_\\mathbf{x}\\, f(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "when every evaluation of $f(\\mathbf{x})$ is *expensive*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example 1**:\n",
    "\n",
    "*Dark art* of selecting the right hyperparameters machine learning model.\n",
    "\n",
    "![Machine learning models often have many hyperparameters.](Figures/ANN_hyperpars.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example 2**:\n",
    "\n",
    "*Model calibration* to find the right parameters or structure of a mechanistic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example 3**:\n",
    "\n",
    "Experimentation for improvement (drug design, product development, new cooking recipes...):\n",
    "\n",
    "1. Gather data\n",
    "2. Make model\n",
    "3. Select new experiments for testing in the lab\n",
    "4. Repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Leading example\n",
    "\n",
    "Damped spring mass system.\n",
    "\n",
    "![Spring with mass, damped](Figures/spring.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Model:\n",
    "\n",
    "$$\n",
    "m\\ddot{y}(t) + \\gamma \\dot{y}(t) + k y(t) = 0\\,,\n",
    "$$\n",
    "\n",
    "with:\n",
    "- $m$: the mass (1 kg)\n",
    "- $\\gamma$: friction parameter (unknown)\n",
    "- $k$: the spring constant (unknown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Can be simulated (here exact solution known)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def damped_oscillator(t_vals, g, k, m=1, sigma=0, x0=1):\n",
    "    if g**2 > 4*k*m:\n",
    "        l1 = (-g**2 + (g**2 - 4*k*m)**0.5) / (2 * m)\n",
    "        l2 = (-g**2 - (g**2 - 4*k*m)**0.5) / (2 * m)\n",
    "        c1, c2 = np.linalg.solve([[1, 1], [l1, l2]], [[x0], [0]]).flatten()\n",
    "        y_vals = c1 * exp(t_vals * l1) + c2 * exp(t_vals * l2)\n",
    "    elif g**2 == 4*k*m:\n",
    "        l1 = - g / (2 * m)\n",
    "        c1, c2 = np.linalg.solve([[1, 0], [l1, 1]], [[x0], [0]]).flatten()\n",
    "        y_vals = c1 * exp(t_vals * l1) + c2 * t_vals * exp(t_vals * l1)\n",
    "    else:\n",
    "        alpha = - g / (2 * m)\n",
    "        beta = (4 * m * k - g**2)**0.5 / (2 * m)\n",
    "        c1, c2 = np.linalg.solve([[1, 0], [alpha, beta]], [[x0], [0]]).flatten()\n",
    "        y_vals = exp(alpha * t_vals) * (c1 * cos(beta * t_vals) + c2 * sin(beta * t_vals))\n",
    "    return y_vals + np.random.randn(len(t_vals)) * sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "t_vals = np.linspace(0, 5, num = 1000)\n",
    "\n",
    "for g in np.logspace(-2, 2, num=4):\n",
    "    for k in np.logspace(-2, 2, num=4):\n",
    "        y_vals = damped_oscillator(t_vals, g=g, k=k)\n",
    "        ax.plot(t_vals, y_vals)\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$y(t)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finding the best model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We perform some experiments, and have 20 noisy measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "m=1  # known\n",
    "kstar=7  # unknown\n",
    "gstar=1  # unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# noisy observations\n",
    "t_meas = np.linspace(0, 5, num=20)\n",
    "y_obs = damped_oscillator(t_meas, g=gstar, k=kstar, m=m, sigma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# true function\n",
    "t_vals = np.linspace(0, 5, num = 1000)\n",
    "y_vals = damped_oscillator(t_vals, g=gstar, k=kstar, m=m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(t_vals, y_vals, label='unknown parametrized function')\n",
    "ax.scatter(t_meas, y_obs, c='r', label='noisy observations')\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$y(t)$')\n",
    "ax.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For a given $\\gamma$ and $k$, calculate the mean squared error:\n",
    "\n",
    "$$\n",
    "MSE(\\gamma, k) = \\frac{1}{20} \\sum_{i=1}^{20} (y(t_i) - \\hat{y}(t_i, \\gamma, k))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Best parameters obtained by:\n",
    "\n",
    "$$\n",
    "\\gamma^*, k^* = \\text{arg min}_{\\gamma, k} MSE(\\gamma, k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def get_mse(g, k):\n",
    "    \"\"\"\n",
    "    Computes the mean squared error for a given gamma and k\n",
    "    \"\"\"\n",
    "    y_sim = damped_oscillator(t_meas, g=g, k=k)\n",
    "    return np.log10(np.mean((y_sim - y_obs)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "g_vals = np.logspace(-2, 2, num=100)\n",
    "k_vals = np.logspace(-2, 2, num=100)\n",
    "\n",
    "mse_pars = np.zeros((len(g_vals), len(k_vals)))\n",
    "for i, g in enumerate(g_vals):\n",
    "    for j, k in enumerate(k_vals):\n",
    "        mse_pars[i, j] = get_mse(g=g, k=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Because the example is simple, we can search the complete space for the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "G, K = np.meshgrid(g_vals, k_vals)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.contourf(K, G, mse_pars.T)\n",
    "ax.set_xlabel('$k$')\n",
    "ax.set_ylabel('$g$')\n",
    "ax.scatter(kstar, gstar, c='y')\n",
    "ax.loglog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What if we only can test a limited number of parameter combinations?\n",
    "\n",
    "Grid search vs random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = np.array([[g, k] for k in np.logspace(-1.5, 1.5, num=4) for g in np.logspace(-1.5, 1.5, num=4)])\n",
    "param_random = 10**np.random.uniform(-2, 2, size=(16, 2))\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "\n",
    "ax0.scatter(param_grid[:,0], param_grid[:,1], c='r')\n",
    "ax0.set_title('Grid')\n",
    "\n",
    "ax1.scatter(param_random[:,0], param_random[:,1], c='r')\n",
    "ax1.set_title('Random')\n",
    "\n",
    "for ax in (ax1, ax0):\n",
    "    ax.scatter(kstar, gstar, c='y')\n",
    "    ax.loglog()\n",
    "    ax.set_xlabel('$k$')\n",
    "    ax.set_ylabel('$\\gamma$')\n",
    "    ax.set_ylim([1e-2, 1e2])\n",
    "    ax.set_xlim([1e-2, 1e2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> **Don't use grid search!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate instances\n",
    "simulated_parameters = param_random\n",
    "#simulated_parameters = param_grid\n",
    "mse_obs = np.array([get_mse(g=g, k=k) for g, k in simulated_parameters])\n",
    "mse_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Surrogate modelling with Gaussian processes\n",
    "\n",
    "Predict the performance of *new* parameter combinations using a surrogate model.\n",
    "\n",
    "(From here on $\\mathbf{x}\\in\\mathcal{X}$ is used to denote the parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Gaussian process:\n",
    "- learn a function of the form $f: \\mathcal{X}\\rightarrow \\mathbb{R}$\n",
    "- nonlinear model, uses a positive-definite covariance function $K:\\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}$\n",
    "- Bayesian method: prior on function and use Bayes' theorem to condition on observed data\n",
    "- can be updated if more date becomes available (online learning)\n",
    "- fully probabilistic, in theory no tuning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The model return for a given instance $\\mathcal{x}$:\n",
    "\n",
    "- $\\mu(\\mathbf{x})$: expected value\n",
    "- $\\sigma(\\mathbf{x})$: standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gaussian_process = GaussianProcessRegressor(alpha=1e-2)\n",
    "gaussian_process.fit(np.log10(simulated_parameters), mse_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "instance = np.log10([[3, 0.1]])\n",
    "\n",
    "mu, sigma = gaussian_process.predict(instance, return_std=True)\n",
    "print('Predicted MSE: {} ({})'.format(mu[0], sigma[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Explore whole parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mu_mse = np.zeros_like(mse_pars)\n",
    "std_mse = np.zeros_like(mse_pars)\n",
    "\n",
    "for i, g in enumerate(g_vals):\n",
    "    for j, k in enumerate(k_vals):\n",
    "        instance = np.log10([[g, k]])\n",
    "        mu, sigma = gaussian_process.predict(instance, return_std=True)\n",
    "        mu_mse[i, j] = mu[:]\n",
    "        std_mse[i, j] = sigma[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "ax0.contourf(K, G, mu_mse.T)\n",
    "ax0.set_title('Estimated MSE')\n",
    "\n",
    "ax1.contourf(K, G, std_mse.T)\n",
    "ax1.set_title('Std MSE')\n",
    "\n",
    "for ax in (ax0, ax1):\n",
    "    ax.set_xlabel('$k$')\n",
    "    ax.set_ylabel('$\\gamma$')\n",
    "    ax.loglog()\n",
    "    ax.scatter(kstar, gstar, c='y')\n",
    "    ax.scatter(simulated_parameters[:,0], simulated_parameters[:,1], c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Slice for $k=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "k_slice = np.column_stack((np.logspace(-2, 2, num=100), [1]*100))\n",
    "# expected mse, std\n",
    "mu, sigma = gaussian_process.predict(np.log10(k_slice), return_std=True)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(k_slice[:,0], mu, c='r', label='expected value')\n",
    "ax.plot(k_slice[:,0], mu-2*sigma, c='r', ls='--', label='95% interval')\n",
    "ax.plot(k_slice[:,0], mu+2*sigma, c='r', ls='--')\n",
    "ax.semilogx()\n",
    "ax.set_xlabel('$\\gamma$')\n",
    "ax.set_ylabel('predicted MSE')\n",
    "ax.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Acquisition functions\n",
    "\n",
    "How to choose points to simulate/test?\n",
    "\n",
    "Acquisition function $a:\\mathcal{X}\\rightarrow\\mathbb{R}^+$, determines how 'interesting' a point is to explore.\n",
    "\n",
    "Choose\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_\\text{next} = \\text{arg max}_\\mathbf{x} a(\\mathbf{x})\\,.\n",
    "$$\n",
    "\n",
    "Trade-off between *exploration* and *exploitation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Probability of Improvement**\n",
    "\n",
    "$$\n",
    "a_\\text{PI}(\\mathbf{x}) = P(f(\\mathbf{x})<f(\\mathbf{x}_\\text{best}) ) = \\Phi(\\gamma(\\mathbf{x}))\\,,\n",
    "$$\n",
    "with $\\Phi(\\cdot)$ the CDF of a standard normal distirbution and \n",
    "$$\n",
    "\\gamma(\\mathbf{x})= \\frac{f(x_\\text{best}) - \\mu(\\mathbf{x})}{\\sigma(\\mathbf{x})}\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_gamma(mu, sigma, fbest):\n",
    "    return (fbest - mu) / sigma\n",
    "\n",
    "def probability_improvement(mu, sigma, fbest=np.min(mse_obs)):\n",
    "    \"\"\"\n",
    "    Calculates probability of improvement\n",
    "    \"\"\"\n",
    "    gamma_values = calculate_gamma(mu, sigma, fbest)\n",
    "    return norm.cdf(gamma_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Expected Improvement**\n",
    "\n",
    "$$\n",
    "a_\\text{EI}(\\mathbf{x}) = E[\\max(f(\\mathbf{x}) - f(\\mathbf{x}_\\text{best}), 0)] = \\sigma(\\mathbf{x})(\\gamma(\\mathbf{x})\\Phi(\\gamma(\\mathbf{x})) + \\phi(\\gamma(\\mathbf{x}))\\,,\n",
    "$$\n",
    "with $\\phi(\\cdot)$ the PDF of a standard normal distirbution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def expected_improvement(mu, sigma, fbest=np.min(mse_obs)):\n",
    "    \"\"\"\n",
    "    Calculates expected improvement\n",
    "    \"\"\"\n",
    "    gamma_values = calculate_gamma(mu, sigma, fbest)\n",
    "    return sigma * (gamma_values * norm.cdf(gamma_values) + norm.pdf(gamma_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**GP Lower Confidence Limit**\n",
    "\n",
    "$$\n",
    "a_\\text{LCB} = \\mu(\\mathbf{x}) - \\kappa \\sigma(\\mathbf{x})\\,,\n",
    "$$\n",
    "with $\\kappa$ a parameter determining the tightness of the bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def lower_confidence_bound(mu, sigma, kappa=2):\n",
    "    \"\"\"\n",
    "    Calculates lower confidence bound\n",
    "    \"\"\"\n",
    "    return mu - kappa * sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_acquisitions():\n",
    "    fig, (ax0, ax1, ax2, ax3) = plt.subplots(nrows=4, sharex=True, figsize=(8, 10))\n",
    "\n",
    "    # plot mu and sigma\n",
    "    ax0.plot(k_slice[:,0], mu, c='r', label='expected value')\n",
    "    ax0.plot(k_slice[:,0], mu-2*sigma, c='r', ls='--', label='95% interval')\n",
    "    ax0.plot(k_slice[:,0], mu+2*sigma, c='r', ls='--')\n",
    "    ax0.semilogx()\n",
    "    ax0.set_ylabel('predicted\\nMSE')\n",
    "\n",
    "    # plot information gains\n",
    "    ax1.plot(k_slice[:,0], probability_improvement(mu, sigma), label='PI')\n",
    "    ax1.set_ylabel('PI')\n",
    "    ax2.plot(k_slice[:,0], expected_improvement(mu, sigma), label='EI')\n",
    "    ax2.set_ylabel('EI')\n",
    "    ax3.plot(k_slice[:,0], lower_confidence_bound(mu, sigma), label='LCB')\n",
    "    ax3.set_ylabel('UCB')\n",
    "    ax3.set_xlabel('$\\gamma$')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_acquisitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "PI = np.zeros_like(mse_pars)\n",
    "EI = np.zeros_like(mse_pars)\n",
    "LCB = np.zeros_like(mse_pars)\n",
    "\n",
    "for i, g in enumerate(g_vals):\n",
    "    for j, k in enumerate(k_vals):\n",
    "        instance = np.log10([[g, k]])\n",
    "        mu, sigma = gaussian_process.predict(instance, return_std=True)\n",
    "        mu = mu[:]\n",
    "        sigma = sigma[:]\n",
    "        PI[i,j] = probability_improvement(mu, sigma)\n",
    "        EI[i,j] = expected_improvement(mu, sigma)\n",
    "        LCB[i,j] = lower_confidence_bound(mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(13, 5))\n",
    "ax0.contourf(K, G, PI.T)\n",
    "ax0.set_title('PI')\n",
    "ax0.scatter(simulated_parameters[:,0], simulated_parameters[:,1], c='r')\n",
    "\n",
    "ax1.contourf(K, G, EI.T)\n",
    "ax1.set_title('EI')\n",
    "ax1.scatter(simulated_parameters[:,0], simulated_parameters[:,1], c='r')\n",
    "\n",
    "ax2.contourf(K, G, LCB.T)\n",
    "ax2.set_title('LCB')\n",
    "ax2.scatter(simulated_parameters[:,0], simulated_parameters[:,1], c='r')\n",
    "\n",
    "for ax in (ax0, ax1, ax2):\n",
    "    ax.set_xlabel('$k$')\n",
    "    ax.set_ylabel('$\\gamma$')\n",
    "    ax.loglog()\n",
    "    ax.scatter(kstar, gstar, c='y')\n",
    "    ax.scatter(simulated_parameters[:,0], simulated_parameters[:,1], c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Use Gaussian process model to select new points to evaluate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Concluding remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Remark 1**\n",
    "\n",
    "Gaussian process model usually continuous and differentiable w.r.t. $\\mathbf{x}$:\n",
    "\n",
    "- $\\nabla \\mu(\\mathbf{x})$\n",
    "- $\\nabla \\sigma(\\mathbf{x})$\n",
    "\n",
    "gradient-based optimization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Ramark 2**\n",
    "\n",
    "Use of correlation between instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "param_random = 10**np.random.uniform(-2, 2, size=(20, 2))\n",
    "\n",
    "mu, cov = gaussian_process.predict(np.log10(param_random), return_cov=True)\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(8, 4))\n",
    "ax0.plot(mu)\n",
    "ax0.set_ylabel('estimated MSE')\n",
    "\n",
    "ax1.imshow(cov, interpolation='nearest', cmap='hot')\n",
    "ax1.set_title('Covariance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Use coviance to select a *set* of informative instances to explore!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Remark 3**\n",
    "\n",
    "Some instances will differ in execution times: \n",
    "\n",
    "- regularization size (machine learning)\n",
    "- learning rate (machine learning)\n",
    "- number of parameters\n",
    "- grid size approximation (mechanistic modelling)\n",
    "\n",
    "Better optimize *expected improvement per second*. Use a model of expected duration (second Gaussian process)>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "Snoek, J., Larochelle, H., Adams, R. '*Practical Bayesian optimization of machine learning algorithms'. Advances in Neural Information Processing Systems (2012)\n",
    "\n",
    "Rasmussen, C., Williams, C., '*Gaussian Processes for Machine Learning*'. The MIT Press (2006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (Anaconda)",
   "language": "python",
   "name": "anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
